{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO v3 Object Detection\n",
    "\n",
    "Let's see how to use the state of the art in object detection! Please make sure to watch the video, there is no code along here, since we can't reasonably train the YOLOv3 network ourself, instead we will use a pre-established version.\n",
    "\n",
    "CODE SOURCE: https://github.com/xiaochus/YOLOv3\n",
    "\n",
    "REFERENCE (for original YOLOv3): \n",
    "\n",
    "        @article{YOLOv3,  \n",
    "              title={YOLOv3: An Incremental Improvement},  \n",
    "              author={J Redmon, A Farhadi },\n",
    "              year={2018} \n",
    "--------\n",
    "\n",
    "-------\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "from model.yolo_model import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(img):\n",
    "    \"\"\"Resize, reduce and expand image.\n",
    "\n",
    "    # Argument:\n",
    "        img: original image.\n",
    "\n",
    "    # Returns\n",
    "        image: ndarray(64, 64, 3), processed image.\n",
    "    \"\"\"\n",
    "    image = cv2.resize(img, (416, 416),\n",
    "                       interpolation=cv2.INTER_CUBIC)\n",
    "    image = np.array(image, dtype='float32')\n",
    "    image /= 255.\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes(file):\n",
    "    \"\"\"Get classes name.\n",
    "\n",
    "    # Argument:\n",
    "        file: classes name for database.\n",
    "\n",
    "    # Returns\n",
    "        class_names: List, classes name.\n",
    "\n",
    "    \"\"\"\n",
    "    with open(file) as f:\n",
    "        class_names = f.readlines()\n",
    "    class_names = [c.strip() for c in class_names]\n",
    "\n",
    "    return class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(image, boxes, scores, classes, all_classes):\n",
    "    \"\"\"Draw the boxes on the image.\n",
    "\n",
    "    # Argument:\n",
    "        image: original image.\n",
    "        boxes: ndarray, boxes of objects.\n",
    "        classes: ndarray, classes of objects.\n",
    "        scores: ndarray, scores of objects.\n",
    "        all_classes: all classes name.\n",
    "    \"\"\"\n",
    "    for box, score, cl in zip(boxes, scores, classes):\n",
    "        x, y, w, h = box\n",
    "\n",
    "        top = max(0, np.floor(x + 0.5).astype(int))\n",
    "        left = max(0, np.floor(y + 0.5).astype(int))\n",
    "        right = min(image.shape[1], np.floor(x + w + 0.5).astype(int))\n",
    "        bottom = min(image.shape[0], np.floor(y + h + 0.5).astype(int))\n",
    "\n",
    "        cv2.rectangle(image, (top, left), (right, bottom), (255, 0, 0), 2)\n",
    "        cv2.putText(image, '{0} {1:.2f}'.format(all_classes[cl], score),\n",
    "                    (top, left - 6),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.6, (0, 0, 255), 1,\n",
    "                    cv2.LINE_AA)\n",
    "\n",
    "        print('class: {0}, score: {1:.2f}'.format(all_classes[cl], score))\n",
    "        print('box coordinate x,y,w,h: {0}'.format(box))\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_image(image, yolo, all_classes):\n",
    "    \"\"\"Use yolo v3 to detect images.\n",
    "\n",
    "    # Argument:\n",
    "        image: original image.\n",
    "        yolo: YOLO, yolo model.\n",
    "        all_classes: all classes name.\n",
    "\n",
    "    # Returns:\n",
    "        image: processed image.\n",
    "    \"\"\"\n",
    "    pimage = process_image(image)\n",
    "\n",
    "    start = time.time()\n",
    "    boxes, classes, scores = yolo.predict(pimage, image.shape)\n",
    "    end = time.time()\n",
    "\n",
    "    print('time: {0:.2f}s'.format(end - start))\n",
    "\n",
    "    if boxes is not None:\n",
    "        draw(image, boxes, scores, classes, all_classes)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_video(video, yolo, all_classes):\n",
    "    \"\"\"Use yolo v3 to detect video.\n",
    "\n",
    "    # Argument:\n",
    "        video: video file.\n",
    "        yolo: YOLO, yolo model.\n",
    "        all_classes: all classes name.\n",
    "    \"\"\"\n",
    "    video_path = os.path.join(\"videos\", \"test\", video)\n",
    "    camera = cv2.VideoCapture(video_path)\n",
    "    cv2.namedWindow(\"detection\", cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "    # Prepare for saving the detected video\n",
    "    sz = (int(camera.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "        int(camera.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mpeg')\n",
    "\n",
    "    \n",
    "    vout = cv2.VideoWriter()\n",
    "    vout.open(os.path.join(\"videos\", \"res\", video), fourcc, 20, sz, True)\n",
    "\n",
    "    while True:\n",
    "        res, frame = camera.read()\n",
    "\n",
    "        if not res:\n",
    "            break\n",
    "\n",
    "        image = detect_image(frame, yolo, all_classes)\n",
    "        cv2.imshow(\"detection\", image)\n",
    "\n",
    "        # Save the video frame by frame\n",
    "        vout.write(image)\n",
    "\n",
    "        if cv2.waitKey(110) & 0xff == 27:\n",
    "                break\n",
    "\n",
    "    vout.release()\n",
    "    camera.release()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yolo = YOLO(0.6, 0.5) initial config\n",
    "yolo = YOLO(0.5, 0.4)\n",
    "file = 'C:/Users/Ruval Lap/Desktop/Jupyter Files/Computer Vision/8-Deep-Learning-Computer-Vision/06-YOLOv3/data/coco_classes.txt'\n",
    "all_classes = get_classes(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 40.49s\n",
      "class: person, score: 0.92\n",
      "box coordinate x,y,w,h: [264.83688354  82.41097927  96.22159004 220.15242577]\n",
      "class: bicycle, score: 0.74\n",
      "box coordinate x,y,w,h: [267.32574463 186.61090851  77.99409866 129.04153347]\n",
      "class: truck, score: 0.95\n",
      "box coordinate x,y,w,h: [335.32653809  61.65903568 302.87967682 134.51803207]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = 'test_0.jpg'\n",
    "path = 'C:/Users/Ruval Lap/Desktop/Jupyter Files/Computer Vision/8-Deep-Learning-Computer-Vision/06-YOLOv3/images/test/'+f\n",
    "image = cv2.imread(path)\n",
    "image = detect_image(image, yolo, all_classes)\n",
    "cv2.imwrite('C:/Users/Ruval Lap/Desktop/Jupyter Files/Computer Vision/8-Deep-Learning-Computer-Vision/06-YOLOv3/images/res/' + f, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting on Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ruval Lap\\Desktop\\Jupyter Files\\Computer Vision\\8-Deep-Learning-Computer-Vision\\06-YOLOv3\n"
     ]
    }
   ],
   "source": [
    "#Change the path\n",
    "import os\n",
    "\n",
    "# change the current working directory\n",
    "os.chdir(\"C:/Users/Ruval Lap/Desktop/Jupyter Files/Computer Vision/8-Deep-Learning-Computer-Vision/06-YOLOv3\")\n",
    "\n",
    "# print the current working directory\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16.04s\n",
      "time: 18.89s\n",
      "time: 20.72s\n",
      "time: 18.38s\n",
      "time: 16.79s\n",
      "time: 17.18s\n",
      "time: 16.82s\n",
      "time: 16.76s\n",
      "time: 20.15s\n",
      "time: 16.84s\n",
      "time: 17.62s\n",
      "time: 18.53s\n",
      "time: 17.21s\n",
      "time: 17.43s\n",
      "time: 17.48s\n",
      "time: 18.72s\n",
      "time: 18.02s\n",
      "time: 19.77s\n",
      "time: 21.24s\n",
      "time: 21.41s\n",
      "time: 20.53s\n",
      "time: 25.91s\n",
      "time: 21.14s\n",
      "time: 20.32s\n",
      "time: 23.03s\n",
      "time: 24.26s\n",
      "time: 23.74s\n",
      "time: 23.20s\n",
      "time: 22.75s\n",
      "time: 21.15s\n",
      "time: 22.72s\n",
      "time: 27.19s\n",
      "time: 20.71s\n",
      "time: 41.73s\n",
      "time: 21.31s\n",
      "time: 22.59s\n",
      "time: 23.58s\n",
      "time: 24.13s\n",
      "time: 23.84s\n",
      "time: 23.95s\n",
      "time: 22.03s\n",
      "time: 22.80s\n",
      "time: 23.89s\n",
      "time: 26.53s\n",
      "time: 25.09s\n",
      "time: 25.09s\n",
      "time: 24.41s\n",
      "time: 27.48s\n",
      "time: 25.51s\n",
      "time: 22.71s\n",
      "time: 25.22s\n",
      "time: 25.55s\n",
      "time: 24.77s\n",
      "time: 23.76s\n",
      "time: 24.07s\n",
      "time: 23.88s\n",
      "time: 24.19s\n",
      "time: 25.78s\n",
      "time: 26.05s\n",
      "time: 30.01s\n",
      "time: 28.14s\n",
      "time: 29.65s\n",
      "time: 33.88s\n",
      "time: 33.73s\n",
      "time: 34.72s\n",
      "time: 35.91s\n",
      "time: 37.47s\n",
      "time: 31.92s\n",
      "time: 34.49s\n",
      "time: 32.27s\n",
      "time: 31.81s\n",
      "time: 32.31s\n",
      "time: 25.12s\n",
      "time: 26.81s\n",
      "time: 27.57s\n",
      "time: 33.33s\n",
      "time: 34.83s\n",
      "time: 28.20s\n",
      "time: 27.53s\n",
      "time: 27.59s\n",
      "time: 27.82s\n",
      "time: 27.31s\n",
      "time: 31.12s\n",
      "time: 27.97s\n",
      "time: 28.52s\n",
      "time: 27.97s\n",
      "time: 28.19s\n",
      "time: 29.31s\n",
      "time: 29.51s\n",
      "time: 29.55s\n",
      "time: 36.13s\n",
      "time: 30.64s\n",
      "time: 33.10s\n",
      "time: 29.65s\n",
      "time: 29.47s\n",
      "time: 30.34s\n",
      "class: bicycle, score: 0.85\n",
      "box coordinate x,y,w,h: [297.80527925  80.85867405 245.15072727 238.71580124]\n",
      "\n",
      "time: 28.11s\n",
      "time: 28.20s\n",
      "class: bicycle, score: 0.67\n",
      "box coordinate x,y,w,h: [311.20438623  72.80386448 242.50996685 253.66839409]\n",
      "\n",
      "time: 30.14s\n",
      "time: 30.70s\n",
      "time: 31.50s\n",
      "time: 29.97s\n",
      "time: 31.30s\n",
      "class: person, score: 0.72\n",
      "box coordinate x,y,w,h: [257.83280659 101.43836975 198.14199734 211.9636631 ]\n",
      "\n",
      "time: 31.81s\n",
      "time: 30.78s\n",
      "class: person, score: 0.80\n",
      "box coordinate x,y,w,h: [259.52191067 102.99217701 213.30948782 208.83187294]\n",
      "\n",
      "time: 30.55s\n",
      "time: 31.33s\n",
      "time: 31.19s\n",
      "time: 34.65s\n",
      "time: 30.75s\n",
      "time: 37.18s\n",
      "time: 32.27s\n",
      "time: 33.36s\n",
      "time: 34.86s\n",
      "time: 39.30s\n",
      "class: person, score: 0.61\n",
      "box coordinate x,y,w,h: [241.6460309   80.03538609 270.25271034 227.31328011]\n",
      "\n",
      "time: 35.03s\n",
      "time: 37.96s\n",
      "time: 36.61s\n",
      "time: 37.61s\n",
      "time: 39.07s\n",
      "time: 35.34s\n",
      "class: person, score: 0.65\n",
      "box coordinate x,y,w,h: [221.65204859  27.98877239 275.27478504 198.86538506]\n",
      "\n",
      "time: 35.63s\n",
      "time: 34.94s\n",
      "time: 35.99s\n",
      "time: 37.86s\n",
      "time: 36.74s\n",
      "time: 34.06s\n",
      "class: bicycle, score: 0.63\n",
      "box coordinate x,y,w,h: [253.22171116  25.86708069 337.451056   259.53486443]\n",
      "\n",
      "time: 33.86s\n",
      "time: 33.97s\n",
      "class: clock, score: 0.76\n",
      "box coordinate x,y,w,h: [ 3.82596884e+02 -3.54566574e-01  2.05476911e+02  2.63208504e+02]\n",
      "\n",
      "time: 34.88s\n",
      "time: 34.49s\n",
      "class: clock, score: 0.75\n",
      "box coordinate x,y,w,h: [381.93747711   6.49946451 214.50436544 233.03239346]\n",
      "\n",
      "time: 34.62s\n",
      "time: 35.33s\n",
      "time: 33.76s\n",
      "time: 35.85s\n",
      "time: 34.74s\n",
      "time: 36.07s\n",
      "time: 34.86s\n",
      "time: 36.14s\n",
      "time: 34.96s\n",
      "time: 37.63s\n",
      "class: clock, score: 0.76\n",
      "box coordinate x,y,w,h: [393.9329567    0.87296963 195.32996607 122.64571667]\n",
      "\n",
      "time: 35.77s\n",
      "time: 35.26s\n",
      "class: clock, score: 0.90\n",
      "box coordinate x,y,w,h: [410.77191114   4.9720788  170.79638386  97.71275282]\n",
      "\n",
      "time: 37.16s\n",
      "class: clock, score: 0.80\n",
      "box coordinate x,y,w,h: [417.08085489   3.77438307 170.79800129  85.05789757]\n",
      "\n",
      "time: 36.86s\n",
      "time: 36.67s\n",
      "time: 35.63s\n",
      "time: 37.15s\n",
      "time: 38.82s\n",
      "time: 38.48s\n",
      "time: 38.74s\n",
      "time: 37.47s\n",
      "time: 37.99s\n",
      "time: 38.04s\n",
      "time: 38.16s\n",
      "time: 39.45s\n",
      "time: 38.05s\n",
      "time: 38.17s\n",
      "time: 38.03s\n",
      "time: 38.51s\n",
      "time: 39.01s\n",
      "time: 38.91s\n",
      "time: 40.39s\n",
      "time: 38.86s\n",
      "time: 39.16s\n",
      "time: 40.65s\n",
      "time: 39.59s\n",
      "time: 39.66s\n",
      "time: 39.78s\n",
      "time: 39.19s\n",
      "time: 39.13s\n",
      "time: 39.85s\n",
      "time: 39.92s\n",
      "time: 41.24s\n",
      "time: 41.05s\n",
      "time: 40.61s\n",
      "time: 40.73s\n",
      "time: 41.63s\n",
      "time: 42.06s\n",
      "time: 42.12s\n",
      "time: 42.97s\n",
      "time: 42.23s\n",
      "time: 43.01s\n",
      "time: 42.99s\n",
      "time: 42.86s\n",
      "time: 43.01s\n",
      "time: 43.92s\n",
      "time: 43.06s\n",
      "time: 44.54s\n",
      "time: 46.87s\n",
      "time: 44.57s\n",
      "time: 43.84s\n",
      "time: 43.50s\n",
      "time: 43.39s\n",
      "time: 43.79s\n",
      "time: 44.05s\n",
      "time: 44.40s\n",
      "time: 45.19s\n",
      "time: 44.90s\n",
      "time: 44.84s\n",
      "time: 45.73s\n",
      "time: 46.83s\n",
      "time: 46.25s\n",
      "time: 46.59s\n",
      "time: 48.56s\n",
      "time: 48.70s\n",
      "time: 47.16s\n",
      "time: 48.22s\n",
      "time: 48.95s\n",
      "time: 49.62s\n",
      "time: 46.71s\n",
      "time: 48.32s\n",
      "time: 48.12s\n",
      "time: 49.86s\n",
      "time: 48.95s\n",
      "time: 49.05s\n",
      "time: 51.26s\n",
      "time: 49.53s\n",
      "time: 49.80s\n",
      "time: 50.61s\n",
      "time: 51.66s\n",
      "time: 51.61s\n",
      "time: 51.53s\n",
      "time: 52.03s\n",
      "time: 52.39s\n",
      "time: 52.52s\n",
      "time: 53.58s\n",
      "time: 53.46s\n",
      "time: 52.96s\n",
      "time: 52.78s\n",
      "time: 52.37s\n",
      "time: 53.64s\n",
      "time: 54.13s\n",
      "time: 53.21s\n",
      "time: 54.48s\n",
      "time: 53.88s\n",
      "time: 54.34s\n",
      "time: 54.08s\n",
      "time: 55.02s\n",
      "time: 54.50s\n",
      "time: 56.77s\n",
      "time: 56.60s\n",
      "time: 57.18s\n",
      "time: 59.37s\n",
      "time: 57.78s\n",
      "time: 55.41s\n",
      "time: 55.87s\n",
      "time: 58.83s\n",
      "time: 57.27s\n",
      "time: 57.92s\n",
      "time: 60.28s\n",
      "time: 58.54s\n",
      "time: 59.13s\n",
      "time: 58.96s\n",
      "time: 58.58s\n",
      "time: 59.26s\n",
      "time: 59.25s\n",
      "time: 59.40s\n",
      "time: 72.78s\n",
      "time: 72.75s\n",
      "time: 64.08s\n",
      "time: 63.40s\n",
      "time: 65.34s\n",
      "time: 65.65s\n",
      "time: 66.31s\n",
      "time: 72.50s\n",
      "time: 75.35s\n",
      "time: 74.26s\n",
      "time: 91.06s\n",
      "time: 78.94s\n",
      "time: 84.16s\n",
      "time: 74.28s\n",
      "time: 87.17s\n",
      "time: 86.68s\n",
      "time: 98.10s\n",
      "time: 83.85s\n",
      "time: 82.18s\n",
      "time: 82.21s\n",
      "time: 85.63s\n",
      "time: 94.86s\n",
      "time: 88.92s\n",
      "time: 85.45s\n",
      "time: 96.42s\n",
      "time: 89.80s\n",
      "time: 98.28s\n",
      "time: 106.29s\n",
      "time: 101.83s\n",
      "time: 91.74s\n",
      "time: 102.97s\n",
      "time: 108.31s\n",
      "time: 108.08s\n",
      "time: 97.20s\n",
      "time: 85.27s\n",
      "time: 78.45s\n",
      "time: 78.28s\n",
      "time: 83.48s\n",
      "time: 80.85s\n",
      "time: 78.85s\n",
      "time: 80.86s\n",
      "time: 79.29s\n",
      "time: 78.78s\n",
      "time: 77.37s\n",
      "time: 78.90s\n",
      "time: 80.18s\n",
      "time: 81.79s\n",
      "time: 79.56s\n",
      "time: 80.93s\n",
      "time: 82.34s\n",
      "time: 80.93s\n",
      "time: 80.33s\n",
      "time: 83.27s\n",
      "time: 81.61s\n",
      "time: 80.31s\n",
      "time: 83.81s\n",
      "time: 83.94s\n",
      "time: 81.54s\n",
      "time: 81.86s\n",
      "time: 83.70s\n",
      "time: 85.62s\n",
      "time: 84.59s\n",
      "time: 85.24s\n",
      "time: 84.62s\n",
      "time: 85.54s\n",
      "time: 86.55s\n",
      "time: 86.16s\n",
      "time: 86.71s\n",
      "time: 91.55s\n",
      "time: 89.94s\n",
      "time: 89.35s\n",
      "time: 93.89s\n",
      "time: 92.25s\n",
      "time: 90.45s\n",
      "time: 90.18s\n",
      "time: 95.43s\n",
      "time: 89.69s\n",
      "time: 91.20s\n",
      "time: 90.32s\n",
      "time: 92.29s\n",
      "time: 92.67s\n",
      "time: 92.39s\n",
      "time: 90.94s\n",
      "time: 91.40s\n",
      "time: 94.30s\n",
      "time: 93.97s\n",
      "time: 101.63s\n",
      "time: 94.57s\n",
      "time: 92.95s\n",
      "time: 94.62s\n",
      "time: 93.94s\n",
      "time: 97.60s\n",
      "time: 94.50s\n",
      "time: 95.80s\n",
      "time: 96.50s\n",
      "time: 96.46s\n",
      "time: 98.49s\n",
      "time: 97.83s\n",
      "time: 96.98s\n",
      "time: 99.12s\n",
      "time: 102.94s\n",
      "time: 99.49s\n",
      "time: 100.62s\n",
      "time: 99.96s\n",
      "time: 102.10s\n",
      "time: 103.74s\n",
      "time: 100.23s\n",
      "time: 102.97s\n",
      "time: 103.87s\n",
      "time: 104.35s\n",
      "time: 100.68s\n",
      "time: 107.51s\n",
      "time: 102.09s\n",
      "time: 105.01s\n",
      "time: 103.75s\n",
      "time: 102.89s\n",
      "time: 104.85s\n",
      "time: 105.88s\n",
      "time: 106.93s\n",
      "time: 105.89s\n",
      "time: 105.62s\n",
      "time: 107.15s\n",
      "time: 107.72s\n",
      "time: 108.54s\n",
      "time: 107.49s\n",
      "time: 108.90s\n",
      "time: 110.65s\n",
      "time: 108.85s\n",
      "time: 119.90s\n",
      "time: 109.35s\n",
      "time: 109.79s\n",
      "time: 110.86s\n",
      "time: 110.63s\n",
      "time: 113.32s\n",
      "time: 114.06s\n",
      "time: 110.40s\n",
      "time: 112.94s\n",
      "time: 113.82s\n",
      "time: 113.81s\n",
      "time: 116.23s\n",
      "time: 114.63s\n",
      "time: 115.90s\n",
      "time: 117.56s\n",
      "time: 114.67s\n",
      "time: 117.98s\n",
      "time: 115.03s\n",
      "time: 117.33s\n",
      "time: 117.36s\n",
      "time: 117.79s\n",
      "time: 121.27s\n",
      "time: 181.37s\n",
      "time: 173.21s\n",
      "time: 186.76s\n",
      "time: 194.42s\n",
      "time: 196.90s\n",
      "time: 210.21s\n",
      "time: 190.42s\n",
      "time: 193.79s\n",
      "time: 237.51s\n",
      "time: 196.94s\n",
      "time: 188.31s\n",
      "time: 171.15s\n",
      "time: 169.02s\n",
      "time: 191.56s\n",
      "time: 210.18s\n",
      "time: 217.89s\n",
      "time: 188.77s\n",
      "time: 169.58s\n",
      "time: 171.02s\n",
      "time: 171.05s\n",
      "time: 171.67s\n",
      "time: 187.56s\n",
      "time: 188.12s\n",
      "time: 187.10s\n",
      "time: 202.35s\n",
      "time: 195.06s\n",
      "time: 177.04s\n",
      "time: 191.30s\n",
      "time: 174.27s\n",
      "time: 186.17s\n",
      "time: 204.70s\n",
      "time: 218.67s\n",
      "time: 201.61s\n",
      "time: 214.02s\n",
      "time: 213.24s\n",
      "time: 221.42s\n",
      "time: 279.36s\n",
      "time: 284.98s\n",
      "time: 251.66s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ruval Lap\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-19-286aaf4a60f0>\", line 3, in <module>\n",
      "    detect_video(video, yolo, all_classes)\n",
      "  File \"<ipython-input-6-77aec999d517>\", line 28, in detect_video\n",
      "    image = detect_image(frame, yolo, all_classes)\n",
      "  File \"<ipython-input-5-61248fe36062>\", line 15, in detect_image\n",
      "    boxes, classes, scores = yolo.predict(pimage, image.shape)\n",
      "  File \"C:\\Users\\Ruval Lap\\Desktop\\Jupyter Files\\Computer Vision\\8-Deep-Learning-Computer-Vision\\06-YOLOv3\\model\\yolo_model.py\", line 195, in predict\n",
      "    boxes, classes, scores = self._yolo_out(outs, shape)\n",
      "  File \"C:\\Users\\Ruval Lap\\Desktop\\Jupyter Files\\Computer Vision\\8-Deep-Learning-Computer-Vision\\06-YOLOv3\\model\\yolo_model.py\", line 144, in _yolo_out\n",
      "    b, c, s = self._process_feats(out, anchors, mask)\n",
      "  File \"C:\\Users\\Ruval Lap\\Desktop\\Jupyter Files\\Computer Vision\\8-Deep-Learning-Computer-Vision\\06-YOLOv3\\model\\yolo_model.py\", line 44, in _process_feats\n",
      "    box_class_probs = K.get_value(K.sigmoid(out[..., 5:]))\n",
      "  File \"C:\\Users\\Ruval Lap\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2377, in get_value\n",
      "    return x.eval(session=get_session())\n",
      "  File \"C:\\Users\\Ruval Lap\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 680, in eval\n",
      "    return _eval_using_default_session(self, feed_dict, self.graph, session)\n",
      "  File \"C:\\Users\\Ruval Lap\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 4951, in _eval_using_default_session\n",
      "    return session.run(tensors, feed_dict)\n",
      "  File \"C:\\Users\\Ruval Lap\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 877, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"C:\\Users\\Ruval Lap\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1100, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"C:\\Users\\Ruval Lap\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1272, in _do_run\n",
      "    run_metadata)\n",
      "  File \"C:\\Users\\Ruval Lap\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1278, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"C:\\Users\\Ruval Lap\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1261, in _run_fn\n",
      "    self._extend_graph()\n",
      "  File \"C:\\Users\\Ruval Lap\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1295, in _extend_graph\n",
      "    tf_session.ExtendSession(self._session)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ruval Lap\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ruval Lap\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\Ruval Lap\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\Ruval Lap\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\Ruval Lap\\anaconda3\\envs\\python-cvcourse\\lib\\inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\Ruval Lap\\anaconda3\\envs\\python-cvcourse\\lib\\inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\Ruval Lap\\anaconda3\\envs\\python-cvcourse\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\Ruval Lap\\anaconda3\\envs\\python-cvcourse\\lib\\inspect.py\", line 732, in getmodule\n",
      "    for modname, module in list(sys.modules.items()):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-286aaf4a60f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mvideo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'video_street_1.mp4'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdetect_video\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myolo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-77aec999d517>\u001b[0m in \u001b[0;36mdetect_video\u001b[1;34m(video, yolo, all_classes)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetect_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myolo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"detection\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-61248fe36062>\u001b[0m in \u001b[0;36mdetect_image\u001b[1;34m(image, yolo, all_classes)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mboxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myolo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Jupyter Files\\Computer Vision\\8-Deep-Learning-Computer-Vision\\06-YOLOv3\\model\\yolo_model.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, image, shape)\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_yolo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mboxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_yolo_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Jupyter Files\\Computer Vision\\8-Deep-Learning-Computer-Vision\\06-YOLOv3\\model\\yolo_model.py\u001b[0m in \u001b[0;36m_yolo_out\u001b[1;34m(self, outs, shape)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m             \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_feats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m             \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filter_boxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Jupyter Files\\Computer Vision\\8-Deep-Learning-Computer-Vision\\06-YOLOv3\\model\\yolo_model.py\u001b[0m in \u001b[0;36m_process_feats\u001b[1;34m(self, out, anchors, mask)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mbox_confidence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbox_confidence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mbox_class_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   2376\u001b[0m     \"\"\"\n\u001b[1;32m-> 2377\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2378\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m    679\u001b[0m     \"\"\"\n\u001b[1;32m--> 680\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[1;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   4950\u001b[0m                        \"graph.\")\n\u001b[1;32m-> 4951\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4952\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1100\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1272\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1273\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1260\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1261\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1294\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1295\u001b[1;33m       \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2043\u001b[0m                         \u001b[1;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2044\u001b[1;33m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2045\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2045\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2046\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[1;32m-> 2047\u001b[1;33m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[0;32m   2048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2049\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1435\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[1;32m-> 1436\u001b[1;33m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[0;32m   1437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1334\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1335\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[1;32m-> 1336\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1337\u001b[0m             )\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Minimal'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[1;32m-> 1193\u001b[1;33m                                                                tb_offset)\n\u001b[0m\u001b[0;32m   1194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1195\u001b[0m         \u001b[0mcolors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mColors\u001b[0m  \u001b[1;31m# just a shorthand + quicker name lookup\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1150\u001b[1;33m         \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1152\u001b[0m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\python-cvcourse\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[1;34m(etype, value, records)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[1;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 451\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# detect videos one at a time in videos/test folder    \n",
    "video = 'video_street_1.mp4'\n",
    "detect_video(video, yolo, all_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
